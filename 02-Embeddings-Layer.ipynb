{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = BASE_DIR\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NUM_WORDS = 1000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>link</th>\n",
       "      <th>published</th>\n",
       "      <th>category</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Watching My Cousin’s Death Go Viral</td>\n",
       "      <td>My cousin Salahuddin was a victim of unforgiva...</td>\n",
       "      <td>The date in the grainy video footage says “Jul...</td>\n",
       "      <td>Bilal Anwar</td>\n",
       "      <td>https://www.buzzfeednews.com/article/bilalanwa...</td>\n",
       "      <td>Mon, 06 Jan 2020 18:56:35 -0500</td>\n",
       "      <td>health</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17 Real Weight Loss Tips From People Who Lost ...</td>\n",
       "      <td>Real tips, real people, and real results that'...</td>\n",
       "      <td>Submissions have been edited for length and cl...</td>\n",
       "      <td>Spencer Althouse</td>\n",
       "      <td>https://www.buzzfeed.com/spenceralthouse/weigh...</td>\n",
       "      <td>Tue, 07 Jan 2020 05:25:37 -0500</td>\n",
       "      <td>health</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I Reviewed Popular Blenders On The Market Toda...</td>\n",
       "      <td>I compared Oster, Ninja, and Vitamix.</td>\n",
       "      <td>The Vitamix blender was given to BuzzFeed for ...</td>\n",
       "      <td>Krista Torres</td>\n",
       "      <td>https://www.buzzfeed.com/kristatorres/i-tested...</td>\n",
       "      <td>Fri, 03 Jan 2020 11:25:46 -0500</td>\n",
       "      <td>health</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Disinformation For Hire: How A New Breed Of PR...</td>\n",
       "      <td>One firm promised to “use every tool and take ...</td>\n",
       "      <td>This story was reported in partnership with th...</td>\n",
       "      <td>Craig Silverman</td>\n",
       "      <td>https://www.buzzfeednews.com/article/craigsilv...</td>\n",
       "      <td>Tue, 07 Jan 2020 11:25:45 -0500</td>\n",
       "      <td>politics</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Disinformation For Hire: How A New Breed Of PR...</td>\n",
       "      <td>One firm promised to “use every tool and take ...</td>\n",
       "      <td>This story was reported in partnership with th...</td>\n",
       "      <td>Craig Silverman</td>\n",
       "      <td>https://www.buzzfeednews.com/article/craigsilv...</td>\n",
       "      <td>Tue, 07 Jan 2020 11:25:45 -0500</td>\n",
       "      <td>tech</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                Watching My Cousin’s Death Go Viral   \n",
       "1  17 Real Weight Loss Tips From People Who Lost ...   \n",
       "2  I Reviewed Popular Blenders On The Market Toda...   \n",
       "3  Disinformation For Hire: How A New Breed Of PR...   \n",
       "4  Disinformation For Hire: How A New Breed Of PR...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  My cousin Salahuddin was a victim of unforgiva...   \n",
       "1  Real tips, real people, and real results that'...   \n",
       "2              I compared Oster, Ninja, and Vitamix.   \n",
       "3  One firm promised to “use every tool and take ...   \n",
       "4  One firm promised to “use every tool and take ...   \n",
       "\n",
       "                                                text            author  \\\n",
       "0  The date in the grainy video footage says “Jul...       Bilal Anwar   \n",
       "1  Submissions have been edited for length and cl...  Spencer Althouse   \n",
       "2  The Vitamix blender was given to BuzzFeed for ...     Krista Torres   \n",
       "3  This story was reported in partnership with th...   Craig Silverman   \n",
       "4  This story was reported in partnership with th...   Craig Silverman   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.buzzfeednews.com/article/bilalanwa...   \n",
       "1  https://www.buzzfeed.com/spenceralthouse/weigh...   \n",
       "2  https://www.buzzfeed.com/kristatorres/i-tested...   \n",
       "3  https://www.buzzfeednews.com/article/craigsilv...   \n",
       "4  https://www.buzzfeednews.com/article/craigsilv...   \n",
       "\n",
       "                         published  category   id  \n",
       "0  Mon, 06 Jan 2020 18:56:35 -0500    health  0.0  \n",
       "1  Tue, 07 Jan 2020 05:25:37 -0500    health  1.0  \n",
       "2  Fri, 03 Jan 2020 11:25:46 -0500    health  2.0  \n",
       "3  Tue, 07 Jan 2020 11:25:45 -0500  politics  2.0  \n",
       "4  Tue, 07 Jan 2020 11:25:45 -0500      tech  2.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('dataset_full.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['title', 'summary', 'author', 'link', 'published', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The date in the grainy video footage says “Jul...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Submissions have been edited for length and cl...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Vitamix blender was given to BuzzFeed for ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This story was reported in partnership with th...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This story was reported in partnership with th...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category\n",
       "0  The date in the grainy video footage says “Jul...    health\n",
       "1  Submissions have been edited for length and cl...    health\n",
       "2  The Vitamix blender was given to BuzzFeed for ...    health\n",
       "3  This story was reported in partnership with th...  politics\n",
       "4  This story was reported in partnership with th...      tech"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dataset.text.values.astype('str')\n",
    "labels = dataset.category.values.astype('str')\n",
    "labels_index = list(np.unique(dataset.category.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27310 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1250, 200)\n",
      "Shape of label tensor: (1250, 5)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test \\\n",
    "    = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val \\\n",
    "    = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros(shape=(num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings.pickle', 'wb') as f:\n",
    "    pickle.dump(embedding_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit`s SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for row in X_train:\n",
    "    arr = []\n",
    "    for foo in row:\n",
    "        arr.extend(embedding_matrix[foo])\n",
    "    X.append(arr)\n",
    "X_train = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for row in X_test:\n",
    "    arr = []\n",
    "    for foo in row:\n",
    "        arr.extend(embedding_matrix[foo])\n",
    "    X.append(arr)\n",
    "X_test = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 20000)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 20000)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vlsnk/anaconda3/envs/fucked-up-data-science/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.192"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        num_words,\n",
    "        EMBEDDING_DIM,\n",
    "        input_shape=(MAX_SEQUENCE_LENGTH,),\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False\n",
    "    ),\n",
    "    layers.Conv1D(128, 5, activation='relu'),\n",
    "    layers.MaxPooling1D(5),\n",
    "    layers.Conv1D(128, 5, activation='relu'),\n",
    "    layers.MaxPooling1D(5),\n",
    "    layers.Conv1D(128, 5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(labels_index), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1814 - acc: 0.9312 - val_loss: 1.7280 - val_acc: 0.6250\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1906 - acc: 0.9237 - val_loss: 1.9146 - val_acc: 0.6250\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1978 - acc: 0.9225 - val_loss: 1.2903 - val_acc: 0.6700\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1664 - acc: 0.9375 - val_loss: 1.4261 - val_acc: 0.6400\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1789 - acc: 0.9388 - val_loss: 1.2504 - val_acc: 0.6700\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1572 - acc: 0.9350 - val_loss: 1.3483 - val_acc: 0.6800\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2886 - acc: 0.9075 - val_loss: 1.3467 - val_acc: 0.6750\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1172 - acc: 0.9463 - val_loss: 2.0091 - val_acc: 0.6450\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1573 - acc: 0.9463 - val_loss: 1.3188 - val_acc: 0.6950\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1768 - acc: 0.9225 - val_loss: 1.7279 - val_acc: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3f4f2e50>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 371us/step\n",
      "Accuracy is 0.6600000262260437\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print('Accuracy is', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
